#!/usr/bin/env python
# coding: utf-8

# In[78]:


import pandas as pd
import matplotlib.pyplot as plt 
import numpy as np 
import seaborn as sns
get_ipython().run_line_magic('matplotlib', 'inline')

from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LinearRegression 
from sklearn import metrics 


# In[79]:


df = pd.read_csv('/Users/JorgeArthur/Documents/winequality-red.csv')
df


# In[292]:


df['quality'].value_counts()


# In[293]:


df.describe()


# In[294]:


df.info()


# In[295]:


# Searching for null values

df.isnull().sum()


# In[296]:


df.columns


# In[297]:


columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
       'pH', 'sulphates', 'alcohol', 'quality']


# ### Ploting box plots 

# In[298]:


for column in columns:
    plt.figure()
    sns.boxplot(x=df[column])
    plt.title(column)


# ### Dealing with outliers

# In[299]:



from scipy import stats
z = np.abs(stats.zscore(df))
z


# In[69]:


threshold = 3
np.where(z > 3)


# In[71]:


df = df[(z < 3).all(axis=1)]
df


# In[72]:


df.describe()


# In[73]:


# Visualizing dataset distribution throughtout columns

df.hist(figsize=(17,12))
plt.show()


# ### Ploting scatter plots

# In[143]:


density = df['density']
ph = df['pH']

plt.scatter(x=density, y=ph)
plt.title('Density vs. pH', fontsize=16)
plt.xlabel('density (water)')
plt.ylabel('pH (wine)')


# In[141]:


fa = df['fixed acidity']
va = df['volatile acidity']

plt.scatter(x=fa, y=va, color = 'black')
plt.title('Acid type proportion', fontsize=16)
plt.xlabel('nonvolotile acid (wine)')
plt.ylabel('acetic acid (wine)')
plt.show()


# In[139]:


fsd = df['free sulfur dioxide']
tsd = df['total sulfur dioxide']

plt.scatter(x=fsd, y=tsd, color = 'red')
plt.title('Sulfur dioxide scatter plot', fontsize=16)
plt.xlabel('free sulfur dioxide(water)')
plt.ylabel('total sulfur dioxide(wine)')
plt.show()


# In[48]:


correlations = df.corr()['quality'].drop('quality')
correlations.sort_values(ascending = False)


# In[65]:


plt.figure(figsize=(10,5))
sns.heatmap(df.corr(), annot = True, cmap = 'coolwarm')


# In[91]:


strong_corr = df[['volatile acidity','citric acid','sulphates','alcohol']]

sns.pairplot(strong_corr)


# ### Feature engineering 

# In[94]:


df['tasty'] = [0 if x < 6 else 1 for x in df['quality']]


# In[104]:


g = sns.jointplot(x="quality", y="volatile acidity", data=df,
           kind="kde", space=0)


# In[134]:


g = sns.jointplot(x= 'fixed acidity',y= 'pH', data=df, kind = 'reg', height = 9, space=1)

plt.title('Fixed Acidity  vs. pH',size=30)
plt.xlabel('Fixed acidity',size=30)
plt.ylabel('pH',size=40)


# ### Ploting bar plot

# In[147]:


sns.catplot(x="quality", y="volatile acidity", hue="tasty", kind="bar", data=df);

plt.title('Tasty & Non-Tasty wine volatile acidity level',size=19)
plt.xlabel('Wine Quality',size=16)
plt.ylabel('Volatile Acidity Level',size=16)


# In[146]:


plt.figure(figsize=(12,8))
sns.boxplot(x="quality", y="sulphates", hue="tasty", data=df )
plt.title("Tasty & Non-Tasty wine sulphate level", fontsize=20)
plt.xlabel("Wine Quality Level",fontsize=16)
plt.ylabel("Sulphate Level", fontsize=16)


# In[145]:


plt.figure(figsize=(12,8))
sns.violinplot(x="quality", y="alcohol", hue="tasty", inner='quartile',data= df )
plt.title("Tasty & Non-Tasty wine percent alcohol content",fontsize=20)
plt.xlabel("Wine Quality Level", fontsize=16)
plt.ylabel("Percent alcohol content ", fontsize=16)


# In[149]:


tasty_wine = df[df['tasty']==1]
tasty_wine.describe()


# In[150]:


not_tasty_wine = df[df['tasty']==0]
not_tasty_wine.describe()


# In[156]:


print(f"(Tasty wine sulphates level: {str(tasty_wine['sulphates'].mean())}.")
print(f"(Non-Tasty wine sulphates level: {str(not_tasty_wine['sulphates'].mean())}.")


# In[158]:


print(f"(Tasty wine alcohol content level: {str(tasty_wine['alcohol'].mean())}.")
print(f"(Non-Tasty Wine Alcohol content level: {str(not_tasty_wine['alcohol'].mean())}.")


# In[161]:


print(f"(Tasty Wine Total Sulfur Dioxide level {str(tasty_wine['total sulfur dioxide'].mean())}.")
print(f"(Non-Tasty Wine Total Sulfur Dioxide level {str(not_tasty_wine['total sulfur dioxide'].mean())}.")


# ### Data preprocessing

# In[165]:


x = df.iloc[:, :-1].values
y = df.iloc[:, -1].values


# In[166]:


# spliting the data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)


# In[168]:


sns.displot(df)


# ### Scaling the data using  Standard Scaler

# In[170]:


from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)


# In[171]:


sns.displot(x_train)


# In[173]:


x_test.shape


# ## Modeling

# ### Method 1: K Nearest Neighbour 

# In[248]:


from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(x_train, y_train)
y_pred1=knn.predict(x_test)
y_pred1


# In[249]:


matrix = confusion_matrix(y_test, y_pred1)
matrix


# In[250]:


from sklearn.metrics import r2_score
r2_score(y_test,y_pred1)


# ### Method 2: Linear Regression

# In[251]:


from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(x_train,y_train)
y_pred1 = regressor.predict(x_test)
y_pred1


# In[253]:


r2_score(y_test,y_pred2)


# ### Polinomial regression

# In[284]:


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures

X2 = df.iloc[:, :-1].values
y2 = df.iloc[:, -1].values

from sklearn.model_selection import train_test_split
x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=1)
poly_reg = PolynomialFeatures(degree=4)
x_poly_train = poly_reg.fit_transform(x_train2)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(x_poly_train, y_train2)
x_poly_test = poly_reg.transform(x_test2)
y_pred2 = lin_reg_2.predict(x_poly_test)
y_pred2



# In[285]:


r2_score(y_test2, y_pred2)


# In[282]:


X3 = df.iloc[:, :-1].values
y3 = df.iloc[:, -1].values

from sklearn.model_selection import train_test_split
x_train3, x_test3, y_train3, y_test3 = train_test_split(X3,y3,test_size = 0.2, random_state = 1)

from sklearn.svm import SVR
regressor3 = SVR(kernel='rbf')
regressor3.fit(x_train3,y_train3)
y_pred3 = regressor3.predict(x_test3)
y_pred3


# In[283]:


r2_score(y_test3,y_pred3)


# ### Decision Tree

# In[286]:


# Assign
X4 = df.iloc[:, :-1].values
y4 = df.iloc[:, -1].values

# Split
from sklearn.model_selection import train_test_split
x_train4, x_test4, y_train4, y_test4 = train_test_split(X4,y4,test_size = 0.2, random_state = 4)
from sklearn.tree import DecisionTreeRegressor
regressor4 = DecisionTreeRegressor(random_state = 0)
regressor4.fit(x_train4,y_train4) # replace by x_train , y_train if we split

y_pred4 = regressor4.predict(x_test4)
y_pred


# In[287]:


from sklearn.metrics import r2_score
r2_score(y_test4,y_pred4)


# In[288]:


x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 3)


# In[289]:


# Assign
X5 = df.iloc[:, :-1].values
y5 = df.iloc[:, -1].values

# Split
from sklearn.model_selection import train_test_split
x_train5, x_test5, y_train5, y_test5 = train_test_split(X5,y5,test_size = 0.2, random_state = 6)


from sklearn.ensemble import RandomForestRegressor
regressor5 = RandomForestRegressor(n_estimators = 10, random_state=0)
regressor5.fit(x_train5,y_train5) # replace by x_train , y_train if we split

y_pred5= regressor5.predict(x_test5)


# In[291]:


r2_score(y_test5,y_pred5)


# In[59]:


test_pred = regressor.predict(x_test)
test_pred


# In[301]:


from xgboost import XGBClassifier

model7 = XGBClassifier(random_state=1)
model7.fit(x_train, y_train)
y_pred7 = model7.predict(x_test)
y_pred7


# In[302]:


from sklearn.metrics import r2_score
r2_score(y_test5,y_pred7)


# In[303]:





# In[ ]:




